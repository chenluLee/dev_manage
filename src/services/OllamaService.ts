import { AIReportRequest, AIReportResponse, AIReportConfig, GenerateReportResult } from '@/types';
import { validateAiConfig } from '@/utils/aiConfigValidation';

/**
 * Ollama APIæœåŠ¡ç±»
 * æä¾›ä¸OllamaæœåŠ¡å™¨çš„é›†æˆåŠŸèƒ½ï¼Œæ”¯æŒAIæŠ¥å‘Šç”Ÿæˆ
 * æ›´æ–°æ—¶é—´: 2025-08-14 è°ƒè¯•ç‰ˆæœ¬
 */
export class OllamaService {
  private static readonly DEFAULT_TIMEOUT = 300000; // 300ç§’è¶…æ—¶
  private static readonly DEFAULT_STREAM = false;

  /**
   * ç”ŸæˆAIæŠ¥å‘Š
   * @param config AIé…ç½®ï¼ˆURLã€æ¨¡å‹ã€æ¸©åº¦ï¼‰
   * @param prompt æç¤ºè¯
   * @returns ç”Ÿæˆç»“æœ
   */
  static async generateReport(
    config: AIReportConfig,
    prompt: string
  ): Promise<GenerateReportResult> {
    try {
      // éªŒè¯é…ç½®
      const configValidation = validateAiConfig(config);
      if (!configValidation.isValid) {
        return {
          success: false,
          error: configValidation.error || 'é…ç½®éªŒè¯å¤±è´¥'
        };
      }

      // éªŒè¯æç¤ºè¯
      if (!prompt?.trim()) {
        return {
          success: false,
          error: 'æç¤ºè¯ä¸èƒ½ä¸ºç©º'
        };
      }

      // æ„é€ è¯·æ±‚
      const requestData: AIReportRequest = {
        model: config.modelName,
        prompt: prompt.trim(),
        temperature: config.temperature,
        stream: OllamaService.DEFAULT_STREAM
      };

      // å‘é€APIè¯·æ±‚
      const response = await OllamaService.callOllamaAPI(config.ollamaUrl, requestData);
      
      if (!response.success) {
        return response;
      }

      return {
        success: true,
        content: response.content
      };

    } catch (error) {
      console.error('ç”ŸæˆAIæŠ¥å‘Šå¤±è´¥:', error);
      return {
        success: false,
        error: error instanceof Error ? error.message : 'æœªçŸ¥é”™è¯¯'
      };
    }
  }

  /**
   * æµ‹è¯•OllamaæœåŠ¡è¿æ¥
   * @param config AIé…ç½®
   * @returns è¿æ¥æµ‹è¯•ç»“æœ
   */
  static async testConnection(config: AIReportConfig): Promise<GenerateReportResult> {
    try {
      // éªŒè¯é…ç½®
      const configValidation = validateAiConfig(config);
      if (!configValidation.isValid) {
        return {
          success: false,
          error: configValidation.error || 'é…ç½®éªŒè¯å¤±è´¥'
        };
      }

      // é¦–å…ˆæµ‹è¯•æœåŠ¡æ˜¯å¦å¯è¾¾ - ä½¿ç”¨ /api/tags ç«¯ç‚¹
      const tagsTestResult = await OllamaService.testServiceAvailability(config.ollamaUrl);
      if (!tagsTestResult.success) {
        return tagsTestResult;
      }

      // æ£€æŸ¥æ¨¡å‹æ˜¯å¦å¯ç”¨
      const modelCheckResult = await OllamaService.checkModelAvailability(config.ollamaUrl, config.modelName);
      if (!modelCheckResult.success) {
        return modelCheckResult;
      }

      // æœ€åæµ‹è¯•å®é™…ç”ŸæˆåŠŸèƒ½
      const testPrompt = 'è¯·å›å¤"è¿æ¥æµ‹è¯•æˆåŠŸ"';
      
      const requestData: AIReportRequest = {
        model: config.modelName,
        prompt: testPrompt,
        temperature: 0.1,
        stream: false
      };

      const response = await OllamaService.callOllamaAPI(config.ollamaUrl, requestData);
      
      if (response.success) {
        return {
          success: true,
          content: 'è¿æ¥æµ‹è¯•æˆåŠŸ'
        };
      }

      return response;

    } catch (error) {
      console.error('è¿æ¥æµ‹è¯•å¤±è´¥:', error);
      return {
        success: false,
        error: error instanceof Error ? error.message : 'è¿æ¥æµ‹è¯•å¤±è´¥'
      };
    }
  }

  /**
   * æµ‹è¯•æœåŠ¡æ˜¯å¦å¯è¾¾ - æ”¯æŒå¤šç§APIæ ¼å¼
   * @param baseUrl æœåŠ¡å™¨URL
   * @returns æœåŠ¡å¯ç”¨æ€§æµ‹è¯•ç»“æœ
   */
  private static async testServiceAvailability(baseUrl: string): Promise<GenerateReportResult> {
    try {
      // å…ˆå°è¯• OpenAI å…¼å®¹çš„ /v1/models ç«¯ç‚¹
      const openaiUrl = OllamaService.buildOpenAIModelsUrl(baseUrl);
      const openaiResult = await OllamaService.tryFetchWithTimeout(openaiUrl);
      
      if (openaiResult.success) {
        return { success: true, content: 'æœåŠ¡å¯è¾¾ (OpenAI API å…¼å®¹)' };
      }

      // å¦‚æœå¤±è´¥ï¼Œå°è¯•æ ‡å‡†çš„ Ollama /api/tags ç«¯ç‚¹
      const ollamaUrl = OllamaService.buildTagsUrl(baseUrl);
      const ollamaResult = await OllamaService.tryFetchWithTimeout(ollamaUrl);
      
      if (ollamaResult.success) {
        return { success: true, content: 'æœåŠ¡å¯è¾¾ (Ollama API)' };
      }

      // ä¸¤ä¸ªç«¯ç‚¹éƒ½å¤±è´¥
      const portSuggestion = OllamaService.getPortSuggestion(baseUrl);
      return {
        success: false,
        error: `æœåŠ¡ä¸å¯è¾¾ - å·²å°è¯• OpenAI å’Œ Ollama API æ ¼å¼${portSuggestion}`
      };

    } catch (error) {
      if (error instanceof Error) {
        // å¤„ç†URLæ„å»ºé”™è¯¯
        if (error.message.includes('æ— æ•ˆçš„æœåŠ¡å™¨URL')) {
          return {
            success: false,
            error: error.message
          };
        }
      }
      
      return {
        success: false,
        error: `æœåŠ¡è¿æ¥å¤±è´¥: ${error instanceof Error ? error.message : 'æœªçŸ¥é”™è¯¯'}`
      };
    }
  }

  /**
   * å°è¯•è·å–URLå¹¶å¤„ç†è¶…æ—¶
   * @param url è¦å°è¯•çš„URL
   * @returns è¯·æ±‚ç»“æœ
   */
  private static async tryFetchWithTimeout(url: string): Promise<{ success: boolean; response?: Response; error?: string }> {
    try {
      const controller = new AbortController();
      const timeoutId = setTimeout(() => {
        controller.abort();
      }, 5000);

      const response = await fetch(url, {
        method: 'GET',
        headers: { 'Content-Type': 'application/json' },
        signal: controller.signal
      });

      clearTimeout(timeoutId);
      
      return { success: response.ok, response };
    } catch (error) {
      if (error instanceof Error) {
        if (error.name === 'AbortError') {
          return { success: false, error: 'è¿æ¥è¶…æ—¶' };
        }
        if (error.message.includes('fetch') || error.message.includes('Failed to fetch')) {
          return { success: false, error: 'è¿æ¥å¤±è´¥' };
        }
      }
      return { success: false, error: error instanceof Error ? error.message : 'æœªçŸ¥é”™è¯¯' };
    }
  }

  /**
   * æ£€æŸ¥æ¨¡å‹æ˜¯å¦å¯ç”¨ - æ”¯æŒå¤šç§APIæ ¼å¼
   * @param baseUrl æœåŠ¡å™¨URL
   * @param modelName æ¨¡å‹åç§°
   * @returns æ¨¡å‹å¯ç”¨æ€§æ£€æŸ¥ç»“æœ
   */
  private static async checkModelAvailability(baseUrl: string, modelName: string): Promise<GenerateReportResult> {
    try {
      // å…ˆå°è¯• OpenAI å…¼å®¹çš„ /v1/models ç«¯ç‚¹
      const openaiUrl = OllamaService.buildOpenAIModelsUrl(baseUrl);
      const openaiResult = await OllamaService.tryFetchWithTimeout(openaiUrl);
      
      if (openaiResult.success && openaiResult.response) {
        const data = await openaiResult.response.json();
        const models = data.data || [];
        
        // OpenAI API æ ¼å¼ï¼šæŸ¥æ‰¾æ¨¡å‹ IDï¼Œæ”¯æŒç²¾ç¡®åŒ¹é…å’Œéƒ¨åˆ†åŒ¹é…
        const modelExists = models.some((model: { id: string }) => {
          return model.id === modelName || 
                 model.id === `${modelName}:latest` ||
                 model.id.startsWith(`${modelName}:`);
        });
        
        if (modelExists) {
          return { success: true, content: 'æ¨¡å‹å¯ç”¨ (OpenAI API)' };
        } else {
          const availableModels = models.map((model: { id: string }) => model.id).join(', ');
          return {
            success: false,
            error: `æ¨¡å‹ '${modelName}' ä¸å­˜åœ¨ã€‚å¯ç”¨æ¨¡å‹: ${availableModels || 'æ— '}`
          };
        }
      }

      // å¦‚æœ OpenAI API å¤±è´¥ï¼Œå°è¯•æ ‡å‡†çš„ Ollama API
      const ollamaUrl = OllamaService.buildTagsUrl(baseUrl);
      const ollamaResult = await OllamaService.tryFetchWithTimeout(ollamaUrl);
      
      if (ollamaResult.success && ollamaResult.response) {
        const data = await ollamaResult.response.json();
        const models = data.models || [];
        
        const modelExists = models.some((model: { name: string }) => model.name === modelName);
        
        if (modelExists) {
          return { success: true, content: 'æ¨¡å‹å¯ç”¨ (Ollama API)' };
        } else {
          const availableModels = models.map((model: { name: string }) => model.name).join(', ');
          return {
            success: false,
            error: `æ¨¡å‹ '${modelName}' ä¸å­˜åœ¨ã€‚å¯ç”¨æ¨¡å‹: ${availableModels || 'æ— '}`
          };
        }
      }

      return {
        success: false,
        error: 'æ— æ³•è·å–æ¨¡å‹åˆ—è¡¨ - API æ ¼å¼ä¸æ”¯æŒ'
      };

    } catch (error) {
      if (error instanceof Error && error.message.includes('æ— æ•ˆçš„æœåŠ¡å™¨URL')) {
        return {
          success: false,
          error: error.message
        };
      }
      
      return {
        success: false,
        error: `æ£€æŸ¥æ¨¡å‹å¯ç”¨æ€§å¤±è´¥: ${error instanceof Error ? error.message : 'æœªçŸ¥é”™è¯¯'}`
      };
    }
  }

  /**
   * è·å–ç«¯å£å»ºè®®ä¿¡æ¯
   * @param baseUrl å½“å‰URL
   * @returns ç«¯å£å»ºè®®å­—ç¬¦ä¸²
   */
  private static getPortSuggestion(baseUrl: string): string {
    try {
      const url = new URL(baseUrl);
      const currentPort = url.port || (url.protocol === 'https:' ? '443' : '80');
      
      if (currentPort !== '11434') {
        return `ã€‚å»ºè®®å°è¯•é»˜è®¤ç«¯å£ 11434: ${url.protocol}//${url.hostname}:11434`;
      } else {
        return 'ã€‚è¯·ç¡®è®¤ Ollama æœåŠ¡å·²å¯åŠ¨ (ollama serve)';
      }
    } catch {
      return 'ã€‚è¯·æ£€æŸ¥ URL æ ¼å¼æ˜¯å¦æ­£ç¡®';
    }
  }

  /**
   * æ„é€  OpenAI å…¼å®¹çš„ Models API URL
   * @param baseUrl åŸºç¡€URL
   * @returns å®Œæ•´çš„ OpenAI Models API URL
   */
  private static buildOpenAIModelsUrl(baseUrl: string): string {
    try {
      // åœ¨å¼€å‘ç¯å¢ƒä¸­ï¼Œæ€»æ˜¯ä½¿ç”¨ä»£ç†è·¯å¾„ä»¥é¿å… CORS é—®é¢˜
      if (import.meta.env.DEV) {
        return '/api/ollama/v1/models';
      }
      
      const url = new URL(baseUrl);
      const pathname = url.pathname.endsWith('/') ? url.pathname : url.pathname + '/';
      url.pathname = pathname + 'v1/models';
      return url.toString();
    } catch (error) {
      throw new Error(`æ— æ•ˆçš„æœåŠ¡å™¨URL: ${baseUrl}`);
    }
  }

  /**
   * æ„é€  Tags API URL (æ ‡å‡† Ollama API)
   * @param baseUrl åŸºç¡€URL
   * @returns å®Œæ•´çš„ Tags API URL
   */
  private static buildTagsUrl(baseUrl: string): string {
    try {
      // åœ¨å¼€å‘ç¯å¢ƒä¸­ï¼Œæ€»æ˜¯ä½¿ç”¨ä»£ç†è·¯å¾„ä»¥é¿å… CORS é—®é¢˜
      if (import.meta.env.DEV) {
        return '/api/ollama/api/tags';
      }
      
      const url = new URL(baseUrl);
      const pathname = url.pathname.endsWith('/') ? url.pathname : url.pathname + '/';
      url.pathname = pathname + 'api/tags';
      return url.toString();
    } catch (error) {
      throw new Error(`æ— æ•ˆçš„æœåŠ¡å™¨URL: ${baseUrl}`);
    }
  }

  /**
   * è°ƒç”¨AI API - æ”¯æŒå¤šç§APIæ ¼å¼
   * @param baseUrl æœåŠ¡å™¨URL
   * @param requestData è¯·æ±‚æ•°æ®
   * @returns APIå“åº”ç»“æœ
   */
  private static async callOllamaAPI(
    baseUrl: string,
    requestData: AIReportRequest
  ): Promise<GenerateReportResult> {
    try {
      // å…ˆå°è¯• OpenAI å…¼å®¹çš„ chat/completions ç«¯ç‚¹
      const openaiResult = await OllamaService.tryOpenAIAPI(baseUrl, requestData);
      if (openaiResult.success) {
        return openaiResult;
      }

      // å¦‚æœå¤±è´¥ï¼Œå°è¯•æ ‡å‡†çš„ Ollama generate ç«¯ç‚¹
      const ollamaResult = await OllamaService.tryOllamaAPI(baseUrl, requestData);
      if (ollamaResult.success) {
        return ollamaResult;
      }

      return {
        success: false,
        error: `APIè°ƒç”¨å¤±è´¥ - OpenAIé”™è¯¯: ${openaiResult.error}, Ollamaé”™è¯¯: ${ollamaResult.error}`
      };

    } catch (error) {
      return {
        success: false,
        error: `APIè°ƒç”¨å¼‚å¸¸: ${error instanceof Error ? error.message : 'æœªçŸ¥é”™è¯¯'}`
      };
    }
  }

  /**
   * å°è¯• OpenAI å…¼å®¹çš„ API è°ƒç”¨
   */
  private static async tryOpenAIAPI(baseUrl: string, requestData: AIReportRequest): Promise<GenerateReportResult> {
    try {
      const url = OllamaService.buildOpenAIChatUrl(baseUrl);
      
      // è½¬æ¢ä¸º OpenAI æ ¼å¼çš„è¯·æ±‚ä½“
      const openaiRequestData = {
        model: requestData.model,
        messages: [
          {
            role: "user",
            content: requestData.prompt
          }
        ],
        temperature: requestData.temperature,
        stream: false
      };

      const controller = new AbortController();
      const timeoutId = setTimeout(() => {
        controller.abort();
      }, OllamaService.DEFAULT_TIMEOUT);

      const response = await fetch(url, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify(openaiRequestData),
        signal: controller.signal
      });

      clearTimeout(timeoutId);

      if (!response.ok) {
        const errorText = await response.text().catch(() => '');
        return {
          success: false,
          error: `HTTP ${response.status}: ${errorText}`
        };
      }

      const responseData = await response.json();
      
      // OpenAI æ ¼å¼çš„å“åº”è§£æ
      if (responseData.choices && responseData.choices[0] && responseData.choices[0].message) {
        return {
          success: true,
          content: responseData.choices[0].message.content
        };
      }

      return {
        success: false,
        error: 'OpenAI API å“åº”æ ¼å¼æ— æ•ˆ'
      };

    } catch (error) {
      if (error instanceof Error) {
        if (error.name === 'AbortError') {
          return { success: false, error: 'è¯·æ±‚è¶…æ—¶' };
        }
        if (error.message.includes('fetch')) {
          return { success: false, error: 'ç½‘ç»œè¿æ¥å¤±è´¥' };
        }
      }
      return {
        success: false,
        error: error instanceof Error ? error.message : 'æœªçŸ¥é”™è¯¯'
      };
    }
  }

  /**
   * å°è¯•æ ‡å‡†çš„ Ollama API è°ƒç”¨
   */
  private static async tryOllamaAPI(baseUrl: string, requestData: AIReportRequest): Promise<GenerateReportResult> {
    try {
      const url = OllamaService.buildApiUrl(baseUrl);
      
      const controller = new AbortController();
      const timeoutId = setTimeout(() => {
        controller.abort();
      }, OllamaService.DEFAULT_TIMEOUT);

      const response = await fetch(url, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify(requestData),
        signal: controller.signal
      });

      clearTimeout(timeoutId);

      if (!response.ok) {
        const errorText = await response.text().catch(() => '');
        return {
          success: false,
          error: `HTTP ${response.status}: ${errorText}`
        };
      }

      const responseData: AIReportResponse = await response.json();
      
      if (!responseData.response) {
        return {
          success: false,
          error: 'Ollama API å“åº”æ ¼å¼æ— æ•ˆ'
        };
      }

      return {
        success: true,
        content: responseData.response
      };

    } catch (error) {
      if (error instanceof Error) {
        if (error.name === 'AbortError') {
          return { success: false, error: 'è¯·æ±‚è¶…æ—¶' };
        }
        if (error.message.includes('fetch')) {
          return { success: false, error: 'ç½‘ç»œè¿æ¥å¤±è´¥' };
        }
      }
      return {
        success: false,
        error: error instanceof Error ? error.message : 'æœªçŸ¥é”™è¯¯'
      };
    }
  }

  /**
   * æ„é€  OpenAI å…¼å®¹çš„ Chat Completions API URL
   * @param baseUrl åŸºç¡€URL
   * @returns å®Œæ•´çš„ OpenAI Chat API URL
   */
  private static buildOpenAIChatUrl(baseUrl: string): string {
    try {
      console.log('ğŸ”§ buildOpenAIChatUrl è°ƒè¯•ä¿¡æ¯:', {
        baseUrl,
        isDev: import.meta.env.DEV,
        mode: import.meta.env.MODE,
        env: import.meta.env
      });
      
      // åœ¨å¼€å‘ç¯å¢ƒä¸­ï¼Œæ€»æ˜¯ä½¿ç”¨ä»£ç†è·¯å¾„ä»¥é¿å… CORS é—®é¢˜
      if (import.meta.env.DEV) {
        const proxyUrl = '/api/ollama/v1/chat/completions';
        console.log('âœ… ä½¿ç”¨ä»£ç†è·¯å¾„:', proxyUrl);
        return proxyUrl;
      }
      
      const url = new URL(baseUrl);
      const pathname = url.pathname.endsWith('/') ? url.pathname : url.pathname + '/';
      url.pathname = pathname + 'v1/chat/completions';
      const directUrl = url.toString();
      console.log('ğŸŒ ä½¿ç”¨ç›´æ¥URL:', directUrl);
      return directUrl;
    } catch (error) {
      console.error('âŒ buildOpenAIChatUrl é”™è¯¯:', error);
      throw new Error(`æ— æ•ˆçš„æœåŠ¡å™¨URL: ${baseUrl}`);
    }
  }

  /**
   * æ„é€ API URL (æ ‡å‡† Ollama API)
   * @param baseUrl åŸºç¡€URL
   * @returns å®Œæ•´çš„API URL
   */
  private static buildApiUrl(baseUrl: string): string {
    try {
      console.log('ğŸ”§ buildApiUrl è°ƒè¯•ä¿¡æ¯:', {
        baseUrl,
        isDev: import.meta.env.DEV,
        mode: import.meta.env.MODE
      });
      
      // åœ¨å¼€å‘ç¯å¢ƒä¸­ï¼Œæ€»æ˜¯ä½¿ç”¨ä»£ç†è·¯å¾„ä»¥é¿å… CORS é—®é¢˜
      if (import.meta.env.DEV) {
        const proxyUrl = '/api/ollama/api/generate';
        console.log('âœ… ä½¿ç”¨ä»£ç†è·¯å¾„:', proxyUrl);
        return proxyUrl;
      }
      
      const url = new URL(baseUrl);
      // ç¡®ä¿è·¯å¾„ä»¥/ç»“å°¾ï¼Œç„¶åæ·»åŠ api/generate
      const pathname = url.pathname.endsWith('/') ? url.pathname : url.pathname + '/';
      url.pathname = pathname + 'api/generate';
      const directUrl = url.toString();
      console.log('ğŸŒ ä½¿ç”¨ç›´æ¥URL:', directUrl);
      return directUrl;
    } catch (error) {
      console.error('âŒ buildApiUrl é”™è¯¯:', error);
      throw new Error(`æ— æ•ˆçš„æœåŠ¡å™¨URL: ${baseUrl}`);
    }
  }

  /**
   * æ£€æŸ¥é…ç½®çš„å®Œæ•´æ€§
   * @param config AIé…ç½®
   * @returns é…ç½®æ˜¯å¦å®Œæ•´
   */
  static isConfigComplete(config: Partial<AIReportConfig> | undefined): config is AIReportConfig {
    if (!config) return false;
    
    return !!(
      config.ollamaUrl &&
      config.modelName &&
      typeof config.temperature === 'number'
    );
  }
}