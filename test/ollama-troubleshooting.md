# Ollama 连接问题排查指南

## ✅ 问题已解决！

根据您的 `curl http://localhost:11345/v1/models` 输出，我们确认了您的配置：

### 您的实际环境
- **服务类型**: OpenAI 兼容的 Ollama 服务
- **端口**: `11345` ✅ 
- **API 格式**: `/v1/models`, `/v1/chat/completions` (OpenAI 兼容)
- **可用模型**: `gpt-oss:latest`, `gpt-oss:120b`, `qwen3:32b`, `qwq:latest`, `deepseek-r1:32b` 等

## 已完成的修复

### 1. 代码适配
- ✅ 支持 OpenAI 兼容 API (`/v1/models`, `/v1/chat/completions`)
- ✅ 支持标准 Ollama API (`/api/tags`, `/api/generate`) 作为备选
- ✅ 智能检测服务类型和模型格式
- ✅ 更新默认配置以匹配您的环境

### 2. 界面优化
- ✅ 更新端口建议 (11345 而不是 11434)
- ✅ 显示您实际可用的模型
- ✅ 添加 OpenAI API 兼容性提示
- ✅ 增强错误提示和故障排除指导

### 3. 测试工具
- ✅ 创建针对您配置的专用测试脚本
- ✅ 通用诊断工具支持多种 API 格式
- ✅ 自动检测和建议正确配置

## 推荐配置

### 最佳配置 (基于您的环境)
- **URL**: `http://localhost:11345`
- **模型**: `gpt-oss` 或 `qwen3:32b`
- **温度**: `0.7`

### 测试您的配置
```bash
# 运行专用测试脚本
node test/test-user-config.js

# 或在浏览器控制台运行
testUserConfig()
```

#### 验证步骤
1. 打开应用设置
2. 在 AI 报告生成部分输入正确配置
3. 点击"测试连接"按钮
4. 查看详细的错误信息和建议

### 4. 故障排除检查清单

- [ ] Ollama 服务已启动 (`ollama serve`)
- [ ] 端口配置正确 (默认 11434)
- [ ] 模型已安装并可用 (`ollama list`)
- [ ] 防火墙允许连接
- [ ] URL 格式正确 (`http://localhost:端口`)

### 5. 获取更多帮助

如果问题仍然存在：

1. **查看详细错误信息**: 新的连接测试会提供具体的错误原因
2. **使用诊断工具**: 运行 `test/ollama-diagnosis.js` 获取完整诊断
3. **检查端口建议**: 错误信息中会包含端口配置建议
4. **模型替代方案**: 如果指定模型不可用，会显示所有可用模型

### 6. 常见错误信息解释

- `服务不可达`: 检查端口和服务状态
- `连接超时`: Ollama 服务可能未启动
- `模型不存在`: 需要安装指定模型
- `无法连接到Ollama服务`: 检查URL和防火墙